{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearningAssignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv1htuSm2dqO"
      },
      "source": [
        "Importing required library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHGdlNpfjhTI"
      },
      "source": [
        "import numpy as np \n",
        "from zipfile import ZipFile\n",
        "import keras.utils as ku\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical \n",
        "from keras.models import Sequential #Initialise our neural network model as a sequential networkfrom\n",
        "from keras.layers import Conv2D #Convolution operation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Activation#Applies activation function\n",
        "from keras.layers import Dropout#Prevents overfitting by randomly converting few outputs to zero\n",
        "from keras.layers import MaxPooling2D # Maxpooling function\n",
        "from keras.layers import Flatten # Converting 2D arrays into a 1D linear vector\n",
        "from keras.layers import Dense # Regular fully connected neural network\n",
        "from keras import optimizers\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU7gE2gu2wQy"
      },
      "source": [
        "Defining and initializing variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt9HMPS6zLe4"
      },
      "source": [
        "\n",
        "DATA_PATH = 'fer2013.csv'\n",
        "TRAIN_DIR = 'data/train'\n",
        "VALID_DIR = 'data/valid'\n",
        "TEST_DIR = 'data/test'\n",
        "MODEL_DIR = '/model'\n",
        "shape_predictor_68_face_landmarks = 'shape_predictor_68_face_landmarks.dat'\n",
        "\n",
        "img_size = 48\n",
        "crop_size = 44\n",
        "\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "num_classes = len(class_names)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5kio90vl_Sm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hxRftpDl-Pj"
      },
      "source": [
        "Loading image DataBase file from specified location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u33lz0zeligr"
      },
      "source": [
        "def load_data(dataset_path):\n",
        "  data = []  \n",
        "  test_data = []  \n",
        "  test_labels = []  \n",
        "  labels =[]\n",
        "  #filename = \"fer2013.csv.zip\"\n",
        "  #with ZipFile(filename, 'r') as zip: \n",
        "  #  zip.extractall()\n",
        "  with open(dataset_path, 'r') as file:      \n",
        "    for line_no, line in enumerate(file.readlines()):          \n",
        "      if 0 < line_no <= 35887:            \n",
        "        curr_class, line, set_type = line.split(',')            \n",
        "        image_data = np.asarray([float(x) for x in line.split()]).reshape(48, 48)            \n",
        "        image_data =image_data.astype(np.uint8)/255.0                        \n",
        "        if (set_type.strip() == 'PrivateTest'):\n",
        "          test_data.append(image_data)              \n",
        "          test_labels.append(curr_class)            \n",
        "        else:              \n",
        "          data.append(image_data)              \n",
        "          labels.append(curr_class)            \n",
        "    test_data = np.expand_dims(test_data, -1)      \n",
        "    test_labels = to_categorical(test_labels, num_classes = 7)      \n",
        "    data = np.expand_dims(data, -1)         \n",
        "    labels = to_categorical(labels, num_classes = 7)          \n",
        "  return np.array(data), np.array(labels), np.array(test_data), np.array(test_labels)\n",
        "\n",
        "    #print(\"Number of images in Training set:\", len(train_data))print(\"Number of images in Test set:\", len(test_data))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNZ9sgEemeVA"
      },
      "source": [
        "Unzipping the image database on root directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VWt3sp95ncI"
      },
      "source": [
        "#def load_data():\n",
        "#  filename = \"fer2013.csv.zip\"\n",
        "#  with ZipFile(filename, 'r') as zip:\n",
        " #   zip.extractall()\n",
        "#load_data()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRMHVsOWmklk"
      },
      "source": [
        "Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L10M7kB3DcvD"
      },
      "source": [
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=6, mode='auto')\n",
        "checkpointer = ModelCheckpoint('Model/weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYkFlh-9m70B"
      },
      "source": [
        "Building ML model fitting/compiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10fppj8sqGej"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 64\n",
        "learn_rate = 0.001\n",
        "model = Sequential()\n",
        "#Layer 1\n",
        "model.add(Conv2D(64, (5, 5), activation='relu', input_shape=(48, 48, 1), kernel_regularizer=l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "\n",
        "#Layer 2\n",
        "model.add(Conv2D(128, (5, 5), padding='same',activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "\n",
        "#Layer 3\n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "\n",
        "#Layer 4\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "adam = optimizers.Adam(learning_rate = learn_rate)\n",
        "#compiling the model\n",
        "model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])    \n",
        "train_data, train_labels, test_data, test_labels = load_data('fer2013.csv')\n",
        "model.fit(train_data, train_labels, epochs = epochs, batch_size = batch_size, \n",
        "          validation_split = 0.1, \n",
        "          shuffle = True, \n",
        "          callbacks=[lr_reducer, checkpointer, early_stopper]\n",
        "          )\n",
        "print(\"Number of images in Training set:\", len(train_data))\n",
        "print(\"Number of images in Test set:\", len(test_data))\n",
        "predicted_test_labels = np.argmax(model.predict(test_data)) np.argmax(model.predict(test_data), axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZfYYJoCnRyJ"
      },
      "source": [
        "Making predictions from built model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dwwi1pErlA5w",
        "outputId": "d070ebcf-1e04-4f42-de28-243d23e268cd"
      },
      "source": [
        "predicted_test_labels = np.argmax(model.predict(test_data), axis=1)\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "print (\"Accuracy score = \", accuracy_score(test_labels, predicted_test_labels))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score =  0.5444413485650599\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}